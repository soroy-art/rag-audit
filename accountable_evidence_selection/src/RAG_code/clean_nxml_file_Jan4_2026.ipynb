{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e46532",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7140889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b0361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- text helpers --------\n",
    "\n",
    "def _norm_ws(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "    return s\n",
    "\n",
    "def _node_text(node) -> str:\n",
    "    \"\"\"All descendant text, minus excessive whitespace.\"\"\"\n",
    "    if node is None:\n",
    "        return \"\"\n",
    "    return _norm_ws(\"\".join(node.itertext()))\n",
    "\n",
    "def _sec_title(sec) -> str:\n",
    "    t = sec.find(\"./title\")\n",
    "    if t is not None:\n",
    "        return _node_text(t)\n",
    "    # some sections use <label> only\n",
    "    lab = sec.find(\"./label\")\n",
    "    if lab is not None:\n",
    "        return _node_text(lab)\n",
    "    return \"\"\n",
    "\n",
    "# tags that are usually not part of narrative text\n",
    "DROP_TAGS = {\n",
    "    \"table-wrap\", \"fig\", \"graphic\", \"media\", \"supplementary-material\",\n",
    "    \"ref-list\", \"ref\", \"xref\", \"fn\", \"fn-group\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fc368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _should_drop_section(sec) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristics to drop non-body narrative sections.\n",
    "    - References are usually in <back><ref-list>\n",
    "    - Appendices often appear as sec-type=\"appendix\" or titles like 'Appendix'\n",
    "    - Boxes / 'sec-type=\"box\"' are often recommendation boxes; drop if you consider them non-core\n",
    "    \"\"\"\n",
    "    sec_type = (sec.get(\"sec-type\") or \"\").lower()\n",
    "    title = _sec_title(sec).lower()\n",
    "\n",
    "    if sec_type in {\"appendix\", \"supplementary-material\", \"supplement\", \"acknowledgments\", \"acknowledgements\", \"references\", \"ref-list\", \"box\"}:\n",
    "        return True\n",
    "\n",
    "    # title-based filters (tune to your needs)\n",
    "    bad_title_tokens = [\n",
    "        \"appendix\", \"supplementary\", \"additional file\", \"electronic supplementary\",\n",
    "        \"acknowledg\", \"funding\", \"author contributions\", \"competing interests\",\n",
    "        \"references\", \"bibliography\"\n",
    "    ]\n",
    "    if any(tok in title for tok in bad_title_tokens):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def _clean_clone(node):\n",
    "    \"\"\"\n",
    "    Clone node and drop unwanted subtrees (fig/table/xref/etc) before text extraction.\n",
    "    \"\"\"\n",
    "    clone = etree.fromstring(etree.tostring(node))\n",
    "    for tag in DROP_TAGS:\n",
    "        for n in clone.findall(f\".//{tag}\"):\n",
    "            n.getparent().remove(n)\n",
    "    return clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9dc6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- main extraction --------\n",
    "\n",
    "def extract_pmc_structured_text(nxml_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      {\n",
    "        \"title\": \"...\",\n",
    "        \"abstract\": [(\"Heading\", \"text...\"), ...] or [(\"\", \"text...\")]\n",
    "        \"body\": [(\"H1 > H2 > ...\", \"text...\"), ...]\n",
    "        \"structured_text\": \"...\\n\"\n",
    "      }\n",
    "    \"\"\"\n",
    "    parser = etree.XMLParser(recover=True, huge_tree=True)\n",
    "    tree = etree.parse(nxml_path, parser)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # --- Title ---\n",
    "    title = _node_text(root.find(\".//front//article-meta//title-group//article-title\"))\n",
    "\n",
    "    # --- Abstract (often structured as <abstract><sec>...) ---\n",
    "    abstract_items = []\n",
    "    abs_node = root.find(\".//front//article-meta//abstract\")\n",
    "    if abs_node is not None:\n",
    "        # if structured\n",
    "        secs = abs_node.findall(\"./sec\")\n",
    "        if secs:\n",
    "            for s in secs:\n",
    "                heading = _sec_title(s)\n",
    "                txt = _node_text(_clean_clone(s))\n",
    "                # remove heading text duplication if it appears in txt\n",
    "                if heading and txt.lower().startswith(heading.lower()):\n",
    "                    txt = _norm_ws(txt[len(heading):])\n",
    "                if _norm_ws(txt):\n",
    "                    abstract_items.append((heading, txt))\n",
    "        else:\n",
    "            txt = _node_text(_clean_clone(abs_node))\n",
    "            if txt:\n",
    "                abstract_items.append((\"\", txt))\n",
    "\n",
    "    # --- Body sections ---\n",
    "    body_items = []\n",
    "\n",
    "    body = root.find(\".//body\")\n",
    "    if body is not None:\n",
    "        # top-level <sec> under body\n",
    "        for top_sec in body.findall(\"./sec\"):\n",
    "            _walk_sections(top_sec, parents=[], out=body_items)\n",
    "\n",
    "    # Compose a single text blob with clear heading demarcation\n",
    "    lines = []\n",
    "    if title:\n",
    "        lines.append(\"# \" + title)\n",
    "        lines.append(\"\")\n",
    "\n",
    "    if abstract_items:\n",
    "        lines.append(\"## Abstract\")\n",
    "        for h, t in abstract_items:\n",
    "            if h:\n",
    "                lines.append(f\"### {h}\")\n",
    "            lines.append(t)\n",
    "            lines.append(\"\")\n",
    "    else:\n",
    "        lines.append(\"## Abstract\")\n",
    "        lines.append(\"(not available)\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    lines.append(\"## Body\")\n",
    "    for path, txt in body_items:\n",
    "        if path:\n",
    "            lines.append(\"### \" + path)\n",
    "        lines.append(txt)\n",
    "        lines.append(\"\")\n",
    "\n",
    "    structured_text = \"\\n\".join(lines).strip()\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract_items,\n",
    "        \"body\": body_items,\n",
    "        \"structured_text\": structured_text,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2926fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _walk_sections(sec, parents, out):\n",
    "    \"\"\"\n",
    "    Recursively traverse <sec> and extract narrative text from each section.\n",
    "    Uses heading path like \"Methods > Assessment\".\n",
    "    \"\"\"\n",
    "    if _should_drop_section(sec):\n",
    "        return\n",
    "\n",
    "    heading = _sec_title(sec)\n",
    "    new_parents = parents + ([heading] if heading else [])\n",
    "\n",
    "    # extract text from this section excluding child <sec> text (to avoid duplication)\n",
    "    # Strategy: take all direct <p> and other block-like children except nested <sec>.\n",
    "    # Also strip tables/figs/xrefs etc.\n",
    "    clone = _clean_clone(sec)\n",
    "\n",
    "    # remove nested sec content from the clone so this node captures only its own paragraphs\n",
    "    for child_sec in clone.findall(\".//sec\"):\n",
    "        child_sec.getparent().remove(child_sec)\n",
    "\n",
    "    # collect paragraphs and simple blocks\n",
    "    parts = []\n",
    "    for child in clone:\n",
    "        if child.tag == \"title\":\n",
    "            continue\n",
    "        if child.tag == \"sec\":\n",
    "            continue\n",
    "        # keep <p>, <list>, etc. as text\n",
    "        txt = _node_text(child)\n",
    "        if txt:\n",
    "            parts.append(txt)\n",
    "\n",
    "    section_text = _norm_ws(\"\\n\".join(parts))\n",
    "\n",
    "    # only emit if there is meaningful text\n",
    "    if section_text:\n",
    "        path = \" > \".join([p for p in new_parents if p])\n",
    "        out.append((path, section_text))\n",
    "\n",
    "    # recurse into real children\n",
    "    for child_sec in sec.findall(\"./sec\"):\n",
    "        _walk_sections(child_sec, new_parents, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b68d0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- example usage --------\n",
    "if __name__ == \"__main__\":\n",
    "    nxml_dir = \"../results/pmc_openaccess_xml_periop_care/nxml_files\"\n",
    "    tei_xml_dir = \"../results/pmc_openaccess_xml_periop_care/str_text_files\"\n",
    "    for file_name in os.listdir(nxml_dir):\n",
    "        nxml_path = os.path.join(nxml_dir, file_name)\n",
    "        dest_file_name= os.path.join(tei_xml_dir, file_name.replace('.nxml', '.txt'))\n",
    "        data = extract_pmc_structured_text(nxml_path)\n",
    "\n",
    "        #print(data[\"structured_text\"])\n",
    "\n",
    "        # optionally save\n",
    "        with open(dest_file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data[\"structured_text\"] + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keyi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
